{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP5046_Lab08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuruojie777/NLP/blob/main/COMP5046_Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezc9rjh_asVG"
      },
      "source": [
        "# Lab08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nA_uGHieevZ"
      },
      "source": [
        "# Statistical Language Model (SLM)\n",
        "\n",
        "A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability $P(w_1, \\ldots, w_m)$ to the whole sequence. One model solution is to make the assumption that the probability distribution for a word depends only on the previous $n$ words. This is known as an n-gram model, or unigram model when $n = 1$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfT2RsFvE3Lc"
      },
      "source": [
        "## Bigrams and Trigrams\n",
        "\n",
        "An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model. Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\"; size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on.\n",
        "\n",
        "For example, the frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4yjU_UcstUy"
      },
      "source": [
        "Let's see how to build a such a model with NLTK. Let's download some Reuters data and inspect it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJ6LDdq-dB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d684b589-8846-4342-b432-9f44287acec5"
      },
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download('reuters')\n",
        "!unzip -qq /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PmoQ501E_Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da635fb7-0833-4f22-eddd-696e74160d40"
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "first_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN',\n",
              " 'EXPORTERS',\n",
              " 'FEAR',\n",
              " 'DAMAGE',\n",
              " 'FROM',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.-',\n",
              " 'JAPAN',\n",
              " 'RIFT',\n",
              " 'Mounting',\n",
              " 'trade',\n",
              " 'friction',\n",
              " 'between',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'And',\n",
              " 'Japan',\n",
              " 'has',\n",
              " 'raised',\n",
              " 'fears',\n",
              " 'among',\n",
              " 'many',\n",
              " 'of',\n",
              " 'Asia',\n",
              " \"'\",\n",
              " 's',\n",
              " 'exporting',\n",
              " 'nations',\n",
              " 'that',\n",
              " 'the',\n",
              " 'row',\n",
              " 'could',\n",
              " 'inflict',\n",
              " 'far',\n",
              " '-',\n",
              " 'reaching',\n",
              " 'economic',\n",
              " 'damage',\n",
              " ',',\n",
              " 'businessmen',\n",
              " 'and',\n",
              " 'officials',\n",
              " 'said',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thzu9ZHb7OvU"
      },
      "source": [
        "Now let's see what the n-grams look like. More details can be found at [bigrams()](https://www.nltk.org/api/nltk.html#nltk.util.bigrams),  [trigrams()](https://www.nltk.org/api/nltk.html#nltk.util.trigrams),  [ngrams()](https://www.nltk.org/api/nltk.html#nltk.util.ngrams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97SmMUjnFKPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86da833e-e299-4d2a-a58d-681589721c91"
      },
      "source": [
        "print(\"bigrams without pad: \", list(bigrams(first_sentence)))\n",
        "\n",
        "print(\"bigrams with pad: \", list(bigrams(first_sentence, pad_left=True, pad_right=True)))\n",
        "\n",
        "print(\"trigrams without pad: \", list(trigrams(first_sentence)))\n",
        "\n",
        "print(\"trigrams with pad: \", list(trigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams without pad:  [('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n",
            "bigrams with pad:  [(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n",
            "trigrams without pad:  [('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n",
            "trigrams with pad:  [(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvl2w18dB_Js"
      },
      "source": [
        "Now, let's build a trigram model using the Reuters corpus. Building a bigram model is completely analogous and easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OBW2sORF1sn"
      },
      "source": [
        "# create a model which contains the trigram counts  \n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzZFpU5-GD66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe9fac4-101c-4e27-da64-e60c7ae024c0"
      },
      "source": [
        "# inspect the counts of some trigrams\n",
        "\n",
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# counts of the sentence starting with \"The\"\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0\n",
            "8839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9AhXXaHGR-y"
      },
      "source": [
        "# convert counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9VxFRs3GVQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e14399e-6f3e-4eed-ddf3-d20e81aaea6e"
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"] )\n",
        "\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# probabilities of the sentence starting with \"The\"\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16154324146501936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalKlJptvh33"
      },
      "source": [
        "Now you have a tri-gram language model. Let's generate some text. The output text is actually really readable!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBv-NscGe9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8e7cb3b2-7537-4a03-ac65-0662ba002902"
      },
      "source": [
        "import random\n",
        "\n",
        "text = [None, None]\n",
        " \n",
        "sentence_finished = False\n",
        "\n",
        "# Keep generating the next word until reaching the end of the sentence\n",
        "while not sentence_finished:\n",
        "    # Randomly select a probability threshold r\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    # Go through the possible w3 conditioned on current w1 and w2\n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        # Accumulate the probability\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        # When the threshold is reached, use the current w3 as the next word to be generated\n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    # If the last two words are None, it will reach the end and stop generating\n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        "\n",
        "# The generated sentence is as follows\n",
        "' '.join([t for t in text if t])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'USP REAL ESTATE INVESTMENT & lt ; MYM > 3RD QTR JUNE FOUR Shr loss eight cts Net profit 3 , 326 vs 266 . 1 mln Revs 3 , 501 , 000 dlrs .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ9YDhpza29T"
      },
      "source": [
        "# Decoding Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgP7L9aowkD1"
      },
      "source": [
        "In NLP tasks such as chatbot, text summarization, and machine translation, the prediction required is a sequence of words.\n",
        "\n",
        "It is common for models developed for these types of problems to output a probability distribution over each word in the vocabulary for each word in the output sequence. It is then left to a decoder process to transform the probabilities into a final sequence of words.\n",
        "\n",
        "Decoding the most likely output sequence involves searching through all the possible output sequences based on their likelihood. The size of the vocabulary is often tens or hundreds of thousands of words, or even millions of words. Therefore, the search problem is exponential in the length of the output sequence and is intractable (NP-complete) to search completely.\n",
        "\n",
        "In practice, heuristic search methods are used to return one or more approximate or “good enough” decoded output sequences for a given prediction.\n",
        "\n",
        "Candidate sequences of words are scored based on their likelihood. It is common to use a greedy search or a beam search to locate candidate sequences of text. We will look at both of these decoding algorithms now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3q9CKB5arfc"
      },
      "source": [
        "## Greedy Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRs0dBDXv-3U"
      },
      "source": [
        "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence. This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
        "\n",
        "We can demonstrate the greedy search approach to decoding with a small contrived example in Python. We can start off with a prediction problem that involves a sequence of 10 words. Each word is predicted as a probability distribution over a vocabulary of 5 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9bVjguBJksg"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SthjwprJfaE"
      },
      "source": [
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09],\n",
        "        [0.01, 0.09, 0.3, 0.4, 0.2],\n",
        "        [0.4, 0.3, 0.2, 0.01, 0.09]]\n",
        "data = array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82tjZn5SwHks"
      },
      "source": [
        "We will assume that the words have been integer encoded, such that the column index can be used to look-up the associated word in the vocabulary. Therefore, the task of decoding becomes the task of selecting a sequence of integers from the probability distributions.\n",
        "\n",
        "The argmax() mathematical function can be used to select the index of an array that has the largest value. We can use this function to select the word index that is most likely at each step in the sequence. This function is provided directly in numpy.\n",
        "\n",
        "The greedy_decoder() function below implements this decoder strategy using the argmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9o2t8bjJveU"
      },
      "source": [
        "# greedy decoder, pick only index for largest probability each row\n",
        "def greedy_decoder(data):\n",
        "    return [argmax(s) for s in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ccJuMoYwVFR"
      },
      "source": [
        "Running the example outputs a sequence of integers that could then be mapped back to words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbo4v9KeJ6um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba2c7257-c2ac-4c27-d8b2-f23c3e45c1b4"
      },
      "source": [
        "#decode seqeunce\n",
        "result = greedy_decoder(data)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 0, 3, 0, 3, 0, 3, 0, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o46exoAnKN80"
      },
      "source": [
        "## Beamsearch Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz5tkvgQw7_K"
      },
      "source": [
        "Another popular heuristic is the beam search that expands upon the greedy search and returns a list of most likely output sequences.\n",
        "\n",
        "Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
        "\n",
        "We do not need to start with random states; instead, we start with the k most likely words as the first step in the sequence. Common beam width values are 1 for a greedy search and values of 5 or 10 for common benchmark problems in machine translation. Larger beam widths result in better performance of a model as the multiple candidate sequences increase the likelihood of better matching a target sequence. This increased performance results in a decrease in decoding speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8CkPBLKZ3Z"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4qTw0jQxVJV"
      },
      "source": [
        "The beam_search_decoder() function below implements the beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5EFT3nKQSa"
      },
      "source": [
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for step,row in enumerate(data):\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]\n",
        "            for j in range(len(row)):\n",
        "                candidate = [seq + [j], score + (-log(row[j])) ]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "                all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\n",
        "        # select k best\n",
        "        sequences = ordered[:k]\n",
        "        \n",
        "        # display the k-best sequences\n",
        "        print(\"The\", str(k), \"best sequences at step \", str(step), \": \")\n",
        "        print(sequences)\n",
        "        print()\n",
        "\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Rro25Dxbu0"
      },
      "source": [
        "We can tie this together with the sample data from the previous section and this time return the 3 most likely sequences. Running the example prints both the integer sequences and their log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-UQOwuKUh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a32156-7b41-4957-d536-c851c1a2991d"
      },
      "source": [
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"The final decoded 3 best sequences: \")\n",
        "for seq in result:\n",
        "    print(seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 3 best sequences at step  0 : \n",
            "[[[3], 0.916290731874155], [[2], 1.2039728043259361], [[4], 1.6094379124341003]]\n",
            "\n",
            "The 3 best sequences at step  1 : \n",
            "[[[3, 0], 1.83258146374831], [[3, 1], 2.120263536200091], [[2, 0], 2.120263536200091]]\n",
            "\n",
            "The 3 best sequences at step  2 : \n",
            "[[[3, 0, 3], 2.748872195622465], [[3, 0, 2], 3.036554268074246], [[3, 1, 3], 3.036554268074246]]\n",
            "\n",
            "The 3 best sequences at step  3 : \n",
            "[[[3, 0, 3, 0], 3.66516292749662], [[3, 0, 3, 1], 3.952844999948401], [[3, 0, 2, 0], 3.952844999948401]]\n",
            "\n",
            "The 3 best sequences at step  4 : \n",
            "[[[3, 0, 3, 0, 3], 4.581453659370775], [[3, 0, 3, 0, 2], 4.869135731822556], [[3, 0, 3, 1, 3], 4.869135731822556]]\n",
            "\n",
            "The 3 best sequences at step  5 : \n",
            "[[[3, 0, 3, 0, 3, 0], 5.49774439124493], [[3, 0, 3, 0, 3, 1], 5.7854264636967105], [[3, 0, 3, 0, 2, 0], 5.785426463696711]]\n",
            "\n",
            "The 3 best sequences at step  6 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3], 6.414035123119085], [[3, 0, 3, 0, 3, 0, 2], 6.701717195570866], [[3, 0, 3, 0, 3, 1, 3], 6.701717195570866]]\n",
            "\n",
            "The 3 best sequences at step  7 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0], 7.33032585499324], [[3, 0, 3, 0, 3, 0, 3, 1], 7.618007927445021], [[3, 0, 3, 0, 3, 0, 2, 0], 7.618007927445021]]\n",
            "\n",
            "The 3 best sequences at step  8 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0, 3], 8.246616586867395], [[3, 0, 3, 0, 3, 0, 3, 1, 3], 8.534298659319175], [[3, 0, 3, 0, 3, 0, 2, 0, 3], 8.534298659319175]]\n",
            "\n",
            "The 3 best sequences at step  9 : \n",
            "[[[3, 0, 3, 0, 3, 0, 3, 0, 3, 0], 9.16290731874155], [[3, 0, 3, 0, 3, 0, 3, 1, 3, 0], 9.45058939119333], [[3, 0, 3, 0, 3, 0, 2, 0, 3, 0], 9.45058939119333]]\n",
            "\n",
            "\n",
            "The final decoded 3 best sequences: \n",
            "[[3, 0, 3, 0, 3, 0, 3, 0, 3, 0], 9.16290731874155]\n",
            "[[3, 0, 3, 0, 3, 0, 3, 1, 3, 0], 9.45058939119333]\n",
            "[[3, 0, 3, 0, 3, 0, 2, 0, 3, 0], 9.45058939119333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRivr820ULSk"
      },
      "source": [
        "#Neural Language Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Lets start building the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKTEsdU_ULSm"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d"
      },
      "source": [
        "Let's use a popular nursery rhyme — “Cat and Her Kittens” as our corpus. A corpus is defined as the collection of text documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP"
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c0a387-d2ac-4ac0-99f8-af19d6f46e65"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])\n",
        "        # The reason we are using log_softmax here is that we want to calculate -log(p) and find the minimum score                    \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, training loss: 3.6634, training acc: 4.17%\n",
            "Epoch: 20, training loss: 3.4445, training acc: 8.33%\n",
            "Epoch: 30, training loss: 2.9920, training acc: 14.58%\n",
            "Epoch: 40, training loss: 2.5200, training acc: 33.33%\n",
            "Epoch: 50, training loss: 2.2226, training acc: 54.17%\n",
            "Epoch: 60, training loss: 1.8891, training acc: 64.58%\n",
            "Epoch: 70, training loss: 1.6188, training acc: 70.83%\n",
            "Epoch: 80, training loss: 1.4057, training acc: 79.17%\n",
            "Epoch: 90, training loss: 1.2183, training acc: 89.58%\n",
            "Epoch: 100, training loss: 1.1009, training acc: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS"
      },
      "source": [
        "For th decoding, let's first practice with beam search with k=1, which does not store the candidates. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2fff9a-479e-46c7-983e-b067f13d03c6"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "\n",
        "# Generate text, currently it only works with k=1 \n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        # if k = 1, len(seed_candidates) will always be 1\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "            # Since it only works with k = 1, we can simply use [0] to get the word id and log(p)\n",
        "            id, s = get_topK(predicted, k)[0]\n",
        "            # get the output word\n",
        "            output_word = ind_to_word(id)\n",
        "            # put the word into the sentence input\n",
        "            # calcualte the accumulated score by -log(p)\n",
        "            successives.append((seed_text + ' ' + output_word, score - s)) \n",
        "\n",
        "        # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "        # Then, make them as the seed_candidate for the next word to predict\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we naughty go to day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUYNu7WBt4V-"
      },
      "source": [
        "Now, let's modify based on the above code to allow k>1, which stores the candidates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk05en7NnuiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78e6543-ec77-44b6-f55d-cff2e05b0c28"
      },
      "source": [
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "   \n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "        for i in range(len(seed_candidates)):\n",
        "            seed_text, score = seed_candidates[i]\n",
        "            token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "            token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "\n",
        "            seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "            predicted = model(seed_input).cpu().detach().numpy()\n",
        "            \n",
        "            # if k>1 , we can't simply use [0] to get the candidates\n",
        "            # instead, we will modify as follows\n",
        "            for id, s in get_topK(predicted, k):\n",
        "                output_word= ind_to_word(id)\n",
        "                successives.append((seed_text + ' ' + output_word, score - s))\n",
        "        ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "        seed_candidates = ordered[:k]\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we naughty go to day\n",
            "we naughty go to day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DVgoVqR7XrG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}